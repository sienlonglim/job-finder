{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from time import perf_counter\n",
    "from functools import wraps\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "file_handler = logging.FileHandler('logs/app.log', mode='a')\n",
    "file_handler.setFormatter(formatter)\n",
    "logger.addHandler(file_handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions, wrappers and request headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'Accept': 'text/html',\n",
    "           'Accept-Language': 'en-US',\n",
    "           'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_2) AppleWebKit/605.1.15 (KHTML, like Gecko) Safari/605.1.15 Version/13.0.4',\n",
    "           'Referer': 'http://www.google.com/'}\n",
    "\n",
    "def timeit(func):\n",
    "    @wraps(func)\n",
    "    def timeit_wrapper(*args, **kwargs):\n",
    "        start_time = perf_counter()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = perf_counter()\n",
    "        total_time = end_time - start_time\n",
    "        logger.info(f'Function {func.__name__} Took {total_time:.4f} seconds - {args} {kwargs}')\n",
    "        return result\n",
    "    return timeit_wrapper\n",
    "\n",
    "@timeit\n",
    "def get_job_links(keyword, start_page, pages):    \n",
    "    '''Every linkedin job search page carries 25 jobs'''\n",
    "    def custom_selector(tag):\n",
    "        return tag.name == \"a\" and tag.has_attr(\"href\") and keyword in tag.get('href')\n",
    "    title = re.sub(' ', '%20', keyword)\n",
    "    keyword = re.sub(' ', '-', keyword) # This is used inside custom_selector's scope   \n",
    "    logger.info(f'Searching for {keyword}')\n",
    "    job_links = []\n",
    "    position = start_page\n",
    "    # currentJobId = None\n",
    "    try:\n",
    "        for page in tqdm(range(pages)):\n",
    "            # url = f\"https://www.linkedin.com/jobs/search/?currentJobId=3693533935&distance=25&geoId=102454443&keywords={title}&origin=JOB_SEARCH_PAGE_KEYWORD_HISTORY&refresh=true&start={position}\"\n",
    "            # url = f\"https://www.linkedin.com/jobs/search/?currentJobId=3693533935&distance=25&geoId=102454443&keywords={title}&currentJobId=3415227738&position=1&pageNum=0&start={position}\" \n",
    "            # if not currentJobId:\n",
    "            #     url = f\"https://www.linkedin.com/jobs/search/?geoId=102454443&keywords={title}&location=Singapore&start={position}\"\n",
    "            # else:\n",
    "            #     url = f\"https://www.linkedin.com/jobs/search/?currentJobId={currentJobId}&geoId=102454443&keywords={title}&location=Singapore&start={position}\"\n",
    "            url = f\"https://www.linkedin.com/jobs/search/?geoId=102454443&keywords={title}&location=Singapore&start={position}\"\n",
    "            response = requests.get(url, headers=headers)\n",
    "            soup = BeautifulSoup(response.text,'html.parser')\n",
    "            tags = soup.find_all(custom_selector)\n",
    "            for tag in tags:\n",
    "                link = tag.get('href')\n",
    "                job_links.append(link)\n",
    "            # Get the last jobid to start on the next page\n",
    "            # currentJobId = re.findall('-([0-9]{6,})\\?', link)[0]\n",
    "            position += 25\n",
    "        logger.indo('Retrieved links: ', len(job_links), '\\n')\n",
    "    except Exception as e:\n",
    "        logger.error(f'Error at page {page}, {e}')\n",
    "    finally:\n",
    "        return job_links\n",
    "    \n",
    "def get_job_info(url, return_soup=False):\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text,'html.parser')\n",
    "    info = {}\n",
    "\n",
    "    # Page title\n",
    "    title = soup.find('title')\n",
    "    if title:\n",
    "        info['company'] = title.text.split(' hiring')[0]\n",
    "\n",
    "    # Job title\n",
    "    job_title = soup.find('h1')\n",
    "    if job_title:\n",
    "        info['job_title'] = job_title.text\n",
    "\n",
    "    # Job level, type (full time etc), sector\n",
    "    criteria = soup.find_all('span', class_=\"description__job-criteria-text description__job-criteria-text--criteria\")\n",
    "    if criteria:\n",
    "        criteria = [x.text.strip(' \\n') for x in criteria]\n",
    "        try:\n",
    "            info['level'] = criteria[0]\n",
    "            info['job_type'] = criteria[1]\n",
    "            info['industry1'] = criteria[2]\n",
    "            info['industry2'] = criteria[3]\n",
    "        except Exception as e:\n",
    "            logger.error(f'{e, criteria, url}')\n",
    "\n",
    "    # Job scope and requirements\n",
    "    descriptions = soup.find(class_ = \"show-more-less-html__markup show-more-less-html__markup--clamp-after-5 relative overflow-hidden\")\n",
    "    if descriptions:\n",
    "        # If list form:\n",
    "        bullets = [li.text for li in descriptions.find_all('li')]\n",
    "        semicolon_splits = descriptions.text.split(';')\n",
    "        newline_splits = descriptions.text.split('\\n')\n",
    "\n",
    "        line_by_line_desc = ''\n",
    "        experience = ''\n",
    "        spark = ''\n",
    "        degree = ''\n",
    "\n",
    "        if len(bullets)>0:\n",
    "            chosen_method = bullets\n",
    "        elif len(semicolon_splits) > len(newline_splits):\n",
    "            logger.info('Fall back to semicolon split')\n",
    "            chosen_method = semicolon_splits\n",
    "        else:\n",
    "            logger.info('Fall back to newline split')\n",
    "            chosen_method = newline_splits\n",
    "        \n",
    "        for desc in chosen_method:\n",
    "            if 'experience' in desc:\n",
    "                experience += desc + '\\n'\n",
    "            if ('PySpark' in desc) or ('Spark' in desc):\n",
    "                spark += desc + '\\n'\n",
    "            if 'degree' in desc:\n",
    "                degree += desc + '\\n'\n",
    "            line_by_line_desc += desc + '\\n'\n",
    "\n",
    "        info['descriptions'] = line_by_line_desc\n",
    "        info['degree'] = degree\n",
    "        info['experience'] = experience   \n",
    "        info['spark'] = spark \n",
    "    else: # Print notification if nothing found\n",
    "        logger.error(f'Found no description for {url}')\n",
    "\n",
    "    info['link'] = url            \n",
    "    \n",
    "    if return_soup:\n",
    "        return info, soup\n",
    "    else:\n",
    "        return info\n",
    "    \n",
    "\n",
    "def process_df(data):\n",
    "    df = pd.DataFrame.from_dict(data, orient='index')\n",
    "    wNulls = len(df)\n",
    "    df = df[~(df['company'].isnull() & df['job_title'].isnull() & df['level'].isnull() & df['descriptions'].isnull())]\n",
    "    logger.info(f'Removed {wNulls - len(df)} empty roles')\n",
    "\n",
    "    # Deduplication, some jobs are similar but have different link maybe due to their different posting time / reposting\n",
    "    subset_duplicates = ['company', 'job_title', 'level', 'job_type', 'degree', 'experience', 'spark', 'descriptions', 'industry1']\n",
    "    wDups = len(df)\n",
    "    df = df.drop_duplicates(subset=subset_duplicates)\n",
    "    logger.info(f'Removed {wDups - len(df)} duplicates')\n",
    "\n",
    "    # Sorting order and values\n",
    "    df = df[['company', 'job_title', 'level', 'job_type', 'experience', 'spark', 'degree', 'descriptions', 'industry1', 'industry2', 'link']]\n",
    "    df = df.sort_values(by=['level', 'spark', 'company', 'job_type'],\n",
    "                        ascending= [True, False, True, True])\n",
    "    logger.info(f'Extracted {len(df)} number of jobs')\n",
    "\n",
    "    return df\n",
    "\n",
    "def append_to_main(main_df_filepath, most_recent_filepaths):\n",
    "    # Appending to existing dataframe\n",
    "    subfolder = 'ignore/'\n",
    "    main_df = pd.read_excel(subfolder+main_df_filepath)\n",
    "    subset_duplicates = ['company', 'job_title', 'level', 'job_type', 'degree', 'experience', 'spark', 'descriptions', 'industry1']\n",
    "    original_rows = len(main_df)\n",
    "    logger.info(f'Original rows: {original_rows}')\n",
    "\n",
    "    if most_recent_filepaths:\n",
    "        for filepath in most_recent_filepaths:\n",
    "            if 'MAIN' not in filepath:\n",
    "                sub_df = pd.read_excel(subfolder+filepath, index_col=0)\n",
    "                # sub_df.insert(2, 'Emailed  / Messaged Recruiter', np.nan)\n",
    "                # sub_df.insert(2, 'Apply', np.nan)\n",
    "                logger.info(f'Read rows: {len(sub_df)}')\n",
    "                main_df = pd.concat([main_df, sub_df])\n",
    "                main_df = main_df.drop_duplicates(subset=subset_duplicates)\n",
    "\n",
    "    else:\n",
    "        excel_filepaths = [file for file in os.listdir('ignore') if file.endswith('xlsx')]\n",
    "        for item in enumerate(excel_filepaths):\n",
    "            print(item)\n",
    "        idx = input(\"Select file index: \")\n",
    "        sub_df = pd.read_excel(subfolder+excel_filepaths[idx], index_col=0)\n",
    "        # sub_df.insert(2, 'Emailed  / Messaged Recruiter', np.nan)\n",
    "        # sub_df.insert(2, 'Apply', np.nan)\n",
    "        logger.info(f'Read rows: {len(sub_df)}')\n",
    "        main_df = pd.concat([main_df, sub_df])\n",
    "        main_df = main_df.drop_duplicates(subset=subset_duplicates)\n",
    "    \n",
    "    logger.info(f'Added rows: {len(main_df)-original_rows}')\n",
    "    \n",
    "    return main_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input job search keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword = 'data analyst'\n",
    "pages = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:06<00:00,  1.26s/it]\n",
      "  6%|▌         | 6/107 [00:06<01:47,  1.07s/it]ERROR:root:Found no description for https://sg.linkedin.com/jobs/view/data-analyst-at-sea-3616637587?refId=4ZwOfkWEHvrhO5LLM9EkHA%3D%3D&trackingId=9dKaT7BlAL8JieihL57vVw%3D%3D&position=7&pageNum=0&trk=public_jobs_jserp-result_search-card\n",
      "  7%|▋         | 7/107 [00:07<01:34,  1.06it/s]ERROR:root:Found no description for https://sg.linkedin.com/jobs/view/data-analyst-marketing-analytics-regional-brand-growth-marketing-at-shopee-3775405705?refId=4ZwOfkWEHvrhO5LLM9EkHA%3D%3D&trackingId=tRoXd25FmSf6stwO5vT8XA%3D%3D&position=8&pageNum=0&trk=public_jobs_jserp-result_search-card\n",
      "  7%|▋         | 8/107 [00:07<01:10,  1.40it/s]ERROR:root:Found no description for https://sg.linkedin.com/jobs/view/data-analyst-sql-python-at-astek-3773645758?refId=4ZwOfkWEHvrhO5LLM9EkHA%3D%3D&trackingId=dw04f9k0xxJKUnd7bfpLpw%3D%3D&position=9&pageNum=0&trk=public_jobs_jserp-result_search-card\n",
      " 14%|█▍        | 15/107 [00:16<01:58,  1.29s/it]INFO:__main__:Fall back to newline split\n",
      " 18%|█▊        | 19/107 [00:22<02:02,  1.39s/it]INFO:__main__:Fall back to semicolon split\n",
      " 23%|██▎       | 25/107 [00:29<01:45,  1.28s/it]ERROR:root:Found no description for https://sg.linkedin.com/jobs/view/data-analyst-singapore-at-yipitdata-3738898017?refId=BcSlRgLVmPd71bWXqSsETQ%3D%3D&trackingId=2eCd1%2Bwh7pAcfepQuduOCg%3D%3D&position=2&pageNum=0&trk=public_jobs_jserp-result_search-card\n",
      " 27%|██▋       | 29/107 [00:34<01:42,  1.31s/it]ERROR:__main__:(IndexError('list index out of range'), ['Mid-Senior level', 'Full-time', 'Information Technology'], 'https://sg.linkedin.com/jobs/view/data-analyst-power-bi-analyst-at-microsoft-operations-pte-ltd-3785249512?refId=BcSlRgLVmPd71bWXqSsETQ%3D%3D&trackingId=MD%2Bsi7HgcmFXEXH7quV11g%3D%3D&position=6&pageNum=0&trk=public_jobs_jserp-result_search-card')\n",
      " 56%|█████▌    | 60/107 [01:15<01:01,  1.30s/it]INFO:__main__:Fall back to newline split\n",
      " 60%|█████▉    | 64/107 [01:21<01:00,  1.40s/it]INFO:__main__:Fall back to semicolon split\n",
      " 68%|██████▊   | 73/107 [01:32<00:40,  1.19s/it]ERROR:__main__:(IndexError('list index out of range'), ['Mid-Senior level', 'Full-time', 'Information Technology'], 'https://sg.linkedin.com/jobs/view/data-analyst-power-bi-analyst-at-microsoft-operations-pte-ltd-3785249512?refId=23GmLfHGxFSmJC0vaGxnGg%3D%3D&trackingId=CW826pYALoOuH6xvZ5LIxA%3D%3D&position=5&pageNum=0&trk=public_jobs_jserp-result_search-card')\n",
      " 76%|███████▌  | 81/107 [01:42<00:31,  1.19s/it]ERROR:root:Found no description for https://sg.linkedin.com/jobs/view/data-analyst-cross-project-at-hoyoverse-3693533935?refId=23GmLfHGxFSmJC0vaGxnGg%3D%3D&trackingId=IMqSYugHzdgpiiCXMwoEVQ%3D%3D&position=18&pageNum=0&trk=public_jobs_jserp-result_search-card\n",
      " 86%|████████▌ | 92/107 [01:56<00:18,  1.26s/it]ERROR:__main__:(IndexError('list index out of range'), ['Mid-Senior level', 'Full-time', 'Information Technology'], 'https://sg.linkedin.com/jobs/view/data-analyst-power-bi-analyst-at-microsoft-operations-pte-ltd-3785249512?refId=Ip7JxbwUCsd6C6daId6lXw%3D%3D&trackingId=nOTLNvTRe%2FHLJ0kauxdvCA%3D%3D&position=5&pageNum=0&trk=public_jobs_jserp-result_search-card')\n",
      " 97%|█████████▋| 104/107 [02:12<00:03,  1.28s/it]ERROR:root:Found no description for https://sg.linkedin.com/jobs/view/data-analyst-global-branding-at-hoyoverse-3693534895?refId=Ip7JxbwUCsd6C6daId6lXw%3D%3D&trackingId=GaRuUBcOtD2josftJ1GwRQ%3D%3D&position=22&pageNum=0&trk=public_jobs_jserp-result_search-card\n",
      "100%|██████████| 107/107 [02:15<00:00,  1.27s/it]\n"
     ]
    }
   ],
   "source": [
    "links = get_job_links(keyword, start_page=0, pages=pages)\n",
    "main = {}\n",
    "logger.info('Getting job info')\n",
    "for index, link in tqdm(enumerate(links), total = len(links), dynamic_ncols =True):\n",
    "    main[index] = get_job_info(link)\n",
    "# for index, link in enumerate(links):\n",
    "#     main[index] = get_job_info(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Removed 6 empty roles\n",
      "INFO:__main__:Removed 59 duplicates\n",
      "INFO:__main__:Extracted 42 number of jobs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>job_title</th>\n",
       "      <th>level</th>\n",
       "      <th>job_type</th>\n",
       "      <th>experience</th>\n",
       "      <th>spark</th>\n",
       "      <th>degree</th>\n",
       "      <th>descriptions</th>\n",
       "      <th>industry1</th>\n",
       "      <th>industry2</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>Shopee</td>\n",
       "      <td>Data Analyst - Marketing Analytics, Regional B...</td>\n",
       "      <td>Associate</td>\n",
       "      <td>Full-time</td>\n",
       "      <td></td>\n",
       "      <td>Develop smart and efficient solutions to repor...</td>\n",
       "      <td></td>\n",
       "      <td>Provide data and insight support for specific ...</td>\n",
       "      <td>Analyst, Marketing, and Information Technology</td>\n",
       "      <td>Technology, Information and Internet</td>\n",
       "      <td>https://sg.linkedin.com/jobs/view/data-analyst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>HCLTech</td>\n",
       "      <td>Data Analyst (SQL/Snowflake)</td>\n",
       "      <td>Associate</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>At least 3 years of data-related working exper...</td>\n",
       "      <td></td>\n",
       "      <td>Bachelor degree from a recognized tertiary ins...</td>\n",
       "      <td>Working closely with business end-users, marke...</td>\n",
       "      <td>Information Technology</td>\n",
       "      <td>IT Services and IT Consulting</td>\n",
       "      <td>https://sg.linkedin.com/jobs/view/data-analyst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>SHIELD</td>\n",
       "      <td>Data Analyst (Risk)</td>\n",
       "      <td>Associate</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>3-5 years of experience as a hands-on analyst...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Analysis of rich user and transaction data to ...</td>\n",
       "      <td>Analyst</td>\n",
       "      <td>Technology, Information and Internet</td>\n",
       "      <td>https://sg.linkedin.com/jobs/view/data-analyst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Starbucks Coffee Singapore</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Associate</td>\n",
       "      <td>Full-time</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>At least a Bachelor degree in Statistics, Math...</td>\n",
       "      <td>Manage end-to-end data projects; identify issu...</td>\n",
       "      <td>Analyst and Information Technology</td>\n",
       "      <td>Food and Beverage Services</td>\n",
       "      <td>https://sg.linkedin.com/jobs/view/data-analyst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>TRITON AI PTE LTD</td>\n",
       "      <td>Shipping Data Analyst</td>\n",
       "      <td>Associate</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>You should have at least 2 years of relevant w...</td>\n",
       "      <td></td>\n",
       "      <td>Bachelor’s degree in data science, statistics,...</td>\n",
       "      <td>Perm, Central\\nYou should have at least 2 year...</td>\n",
       "      <td>Analyst</td>\n",
       "      <td>Technology, Information and Internet</td>\n",
       "      <td>https://sg.linkedin.com/jobs/view/shipping-dat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       company  \\\n",
       "52                      Shopee   \n",
       "13                     HCLTech   \n",
       "40                      SHIELD   \n",
       "2   Starbucks Coffee Singapore   \n",
       "34           TRITON AI PTE LTD   \n",
       "\n",
       "                                            job_title      level   job_type  \\\n",
       "52  Data Analyst - Marketing Analytics, Regional B...  Associate  Full-time   \n",
       "13                       Data Analyst (SQL/Snowflake)  Associate  Full-time   \n",
       "40                                Data Analyst (Risk)  Associate  Full-time   \n",
       "2                                       Data Analyst   Associate  Full-time   \n",
       "34                              Shipping Data Analyst  Associate  Full-time   \n",
       "\n",
       "                                           experience  \\\n",
       "52                                                      \n",
       "13  At least 3 years of data-related working exper...   \n",
       "40   3-5 years of experience as a hands-on analyst...   \n",
       "2                                                       \n",
       "34  You should have at least 2 years of relevant w...   \n",
       "\n",
       "                                                spark  \\\n",
       "52  Develop smart and efficient solutions to repor...   \n",
       "13                                                      \n",
       "40                                                      \n",
       "2                                                       \n",
       "34                                                      \n",
       "\n",
       "                                               degree  \\\n",
       "52                                                      \n",
       "13  Bachelor degree from a recognized tertiary ins...   \n",
       "40                                                      \n",
       "2   At least a Bachelor degree in Statistics, Math...   \n",
       "34  Bachelor’s degree in data science, statistics,...   \n",
       "\n",
       "                                         descriptions  \\\n",
       "52  Provide data and insight support for specific ...   \n",
       "13  Working closely with business end-users, marke...   \n",
       "40  Analysis of rich user and transaction data to ...   \n",
       "2   Manage end-to-end data projects; identify issu...   \n",
       "34  Perm, Central\\nYou should have at least 2 year...   \n",
       "\n",
       "                                         industry1  \\\n",
       "52  Analyst, Marketing, and Information Technology   \n",
       "13                          Information Technology   \n",
       "40                                         Analyst   \n",
       "2               Analyst and Information Technology   \n",
       "34                                         Analyst   \n",
       "\n",
       "                               industry2  \\\n",
       "52  Technology, Information and Internet   \n",
       "13         IT Services and IT Consulting   \n",
       "40  Technology, Information and Internet   \n",
       "2             Food and Beverage Services   \n",
       "34  Technology, Information and Internet   \n",
       "\n",
       "                                                 link  \n",
       "52  https://sg.linkedin.com/jobs/view/data-analyst...  \n",
       "13  https://sg.linkedin.com/jobs/view/data-analyst...  \n",
       "40  https://sg.linkedin.com/jobs/view/data-analyst...  \n",
       "2   https://sg.linkedin.com/jobs/view/data-analyst...  \n",
       "34  https://sg.linkedin.com/jobs/view/shipping-dat...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = process_df(main)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_recent_file = f\"{keyword}_{datetime.now().strftime('%Y-%m-%d-%H%M')}.xlsx\"\n",
    "df.to_excel(f\"ignore/{most_recent_file}\", engine='xlsxwriter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data analyst_2023-12-22-1425.xlsx', 'data analyst_2023-12-22-1427.xlsx']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_recent_filepaths = [file for file in os.listdir('ignore') if datetime.now().strftime('%Y-%m-%d') in file]\n",
    "most_recent_filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Building new dataframe\n",
    "# main_df = pd.DataFrame(columns = ['company', 'job_title', 'level', 'job_type', 'experience', 'spark', 'degree', 'descriptions', 'industry1', 'industry2', 'link'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Original rows: 111\n",
      "INFO:__main__:Read rows: 42\n",
      "INFO:__main__:Read rows: 42\n",
      "INFO:__main__:Added rows: 8\n"
     ]
    }
   ],
   "source": [
    "main_df = append_to_main('MAIN_2023-12-20.xlsx', most_recent_filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "if f\"MAIN_{datetime.now().strftime('%Y-%m-%d')}.xlsx\" in os.listdir('ignore'):\n",
    "    pass\n",
    "else:\n",
    "    main_df.to_excel(f\"ignore/MAIN_{datetime.now().strftime('%Y-%m-%d')}.xlsx\", engine='xlsxwriter')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
