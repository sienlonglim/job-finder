{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "from time import perf_counter\n",
    "from functools import wraps\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "file_handler = logging.FileHandler('logs/app.log', mode='a')\n",
    "file_handler.setFormatter(formatter)\n",
    "logger.addHandler(file_handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions, wrappers and request headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'Accept': 'text/html',\n",
    "           'Accept-Language': 'en-US',\n",
    "           'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_2) AppleWebKit/605.1.15 (KHTML, like Gecko) Safari/605.1.15 Version/13.0.4',\n",
    "           'Referer': 'http://www.google.com/'}\n",
    "\n",
    "headers['User-Agent'] = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36' # My actual user agent\n",
    "\n",
    "\n",
    "def timeit(func):\n",
    "    @wraps(func)\n",
    "    def timeit_wrapper(*args, **kwargs):\n",
    "        start_time = perf_counter()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = perf_counter()\n",
    "        total_time = end_time - start_time\n",
    "        logger.info(f'Function {func.__name__} Took {total_time:.4f} seconds - {args} {kwargs}')\n",
    "        return result\n",
    "    return timeit_wrapper\n",
    "\n",
    "@timeit\n",
    "def get_job_links(keyword: str, start_page: int, pages: int)-> tuple:    \n",
    "    '''Function to retrieve all job links over specified number of pages and search\n",
    "    Inputs:\n",
    "        keyword: str - job title and other keywords\n",
    "        start_page: int - page to start\n",
    "        pages: int - number of pages to retrieve\n",
    "    Returns \n",
    "        list of job links\n",
    "        list of actual urls used\n",
    "    '''\n",
    "    def custom_selector(tag):\n",
    "        '''\n",
    "        Helper function used to identify a href tag with belongs to the job link\n",
    "        Inputs:\n",
    "            soup tag\n",
    "        Returns soup tag\n",
    "        '''\n",
    "        return tag.name == \"a\" and tag.has_attr(\"href\") and keyword in tag.get('href')\n",
    "    \n",
    "    title = re.sub(' ', '%20', keyword.lower()) # This is used for building url\n",
    "    keyword = re.sub(' ', '-', keyword.lower()) # This is used inside custom_selector's scope   \n",
    "\n",
    "    logger.info(f'Searching for {keyword}')\n",
    "    job_links = []\n",
    "    redirected_urls = []\n",
    "    position = start_page\n",
    "    currentJobId = None\n",
    "    try:\n",
    "        for page in tqdm(range(pages)):\n",
    "            # url = f\"https://www.linkedin.com/jobs/search/?currentJobId=3693533935&distance=25&geoId=102454443&keywords={title}&origin=JOB_SEARCH_PAGE_KEYWORD_HISTORY&refresh=true&start={position}\"\n",
    "            # url = f\"https://www.linkedin.com/jobs/search/?currentJobId=3693533935&distance=25&geoId=102454443&keywords={title}&currentJobId=3415227738&position=1&pageNum=0&start={position}\" \n",
    "            if not currentJobId:\n",
    "                url = f\"https://www.linkedin.com/jobs/search/?geoId=102454443&keywords={title}&location=Singapore&start={position}\"\n",
    "            else:\n",
    "                position = 0\n",
    "                url = f\"https://www.linkedin.com/jobs/search/?currentJobId={currentJobId}&geoId=102454443&keywords={title}&location=Singapore&start={position}\"\n",
    "            # url = f\"https://www.linkedin.com/jobs/search/?geoId=102454443&keywords={title}&location=Singapore&start={position}\"\n",
    "            response = requests.get(url, headers=headers)\n",
    "            redirected_urls.append(response.url) # Get the actual url in case of checking\n",
    "            soup = BeautifulSoup(response.text,'html.parser')\n",
    "            tags = soup.find_all(custom_selector)\n",
    "            for tag in tags:\n",
    "                link = tag.get('href')\n",
    "                link = link.split('?')[0] # Tidy up the link to remove the trackingid\n",
    "                job_links.append(link)\n",
    "\n",
    "            # Get the last jobid to start on the next page\n",
    "            currentJobId = re.findall('-([0-9]{6,})', link)[0]\n",
    "            logger.info(f'LastJobId found: {currentJobId}')\n",
    "            position += 25\n",
    "        logger.info(f'Retrieved links: {len(job_links)}')\n",
    "    except Exception as e:\n",
    "        logger.error(f'Error at page {page}, {e}')\n",
    "    finally:\n",
    "        return job_links, redirected_urls\n",
    "    \n",
    "def get_job_info(url: str, index: int, return_soup: bool=False):\n",
    "    '''\n",
    "    Function to retrieve information for an individual job page\n",
    "    Inputs:\n",
    "        url: str - url for job page\n",
    "        index: int - index of file for tracking\n",
    "        return_soup: bool - whether to return the soup object for debugging\n",
    "    Returns\n",
    "        dict\n",
    "    '''\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text,'html.parser')\n",
    "    info = {}\n",
    "\n",
    "    # JobID\n",
    "\n",
    "    jobid = re.findall('-(\\d{8,})', url) # removed [\\?/] from pattern because of url cleaning ? away in new version\n",
    "    if len(jobid) == 1 :\n",
    "        info['job_id'] = jobid[0]\n",
    "    else:\n",
    "        logger.error(f'Index {index}: Found no Job ID or multiple Job IDs for {url}')\n",
    "\n",
    "    # Page title\n",
    "    title = soup.find('title')\n",
    "    if title:\n",
    "        info['company'] = title.text.split(' hiring')[0]\n",
    "    else:\n",
    "        logger.error(f'Index {index}: Found no company name for {url}')\n",
    "\n",
    "    # Job title\n",
    "    job_title = soup.find('h1')\n",
    "    if job_title:\n",
    "        info['job_title'] = job_title.text\n",
    "    else:\n",
    "        logger.error(f'Index {index}: Found no job title for {url}')\n",
    "\n",
    "    # Job level, type (full time etc), sector\n",
    "    criteria = soup.find_all('span', class_=\"description__job-criteria-text description__job-criteria-text--criteria\")\n",
    "    if criteria:\n",
    "        criteria = [x.text.strip(' \\n') for x in criteria]\n",
    "        try:\n",
    "            info['level'] = criteria[0]\n",
    "            info['job_type'] = criteria[1]\n",
    "            info['industry1'] = criteria[2]\n",
    "            info['industry2'] = criteria[3]\n",
    "        except Exception as e:\n",
    "            logger.error(f'Index {index}: {e, criteria, url}')\n",
    "\n",
    "    # Job scope and requirements\n",
    "    descriptions = soup.find(class_ = \"show-more-less-html__markup show-more-less-html__markup--clamp-after-5 relative overflow-hidden\")\n",
    "    if descriptions:\n",
    "        # Create empty strings to append to\n",
    "        line_by_line_desc = ''\n",
    "        experience = ''\n",
    "        spark = ''\n",
    "        degree = ''\n",
    "\n",
    "        for tag in descriptions.children: # children is a generator of all the tags contained, the alternative to the full tags is .content\n",
    "            description = tag.text.strip('\\n')\n",
    "            if (tag.name == 'ul') or (tag.name == 'ol'):\n",
    "                line_by_line_desc += '\\u2022' + description + '\\n'\n",
    "            elif tag.name == 'br':\n",
    "                continue\n",
    "            else:\n",
    "                line_by_line_desc += description + '\\n'\n",
    "\n",
    "            # Search for special info interested in\n",
    "            if 'experience' in description:\n",
    "                experience += '\\u2022' + description + '\\n'\n",
    "            if ('PySpark' in description) or ('Spark' in description):\n",
    "                spark += '\\u2022' + description + '\\n'\n",
    "            if 'degree' in description:\n",
    "                degree += '\\u2022' + description + '\\n'\n",
    "\n",
    "            info['descriptions'] = line_by_line_desc\n",
    "            info['degree'] = degree\n",
    "            info['experience'] = experience   \n",
    "            info['spark'] = spark \n",
    "    else: # Print notification if nothing found\n",
    "        logger.error(f'Index {index}: Found no description for {url}')\n",
    "\n",
    "    info['link'] = url            \n",
    "    \n",
    "    if return_soup:\n",
    "        return info, soup\n",
    "    else:\n",
    "        return info, None\n",
    "    \n",
    "def process_df(data: dict, remove_nulls: bool=True, remove_duplicates: bool=True)-> pd.DataFrame:\n",
    "    '''\n",
    "    Function to process the dictionaries into a Pandas DataFrame\n",
    "    Inputs:\n",
    "        data: dict - dictionary with information fields\n",
    "    Returns:\n",
    "        pd.DataFrame - dataframe which is deduplicated, and sorted\n",
    "    '''\n",
    "    df = pd.DataFrame.from_dict(data, orient='index')\n",
    "    df = df.set_index('job_id')\n",
    "\n",
    "    if remove_nulls:\n",
    "        wNulls = len(df)\n",
    "        # df = df[~(df['company'].isnull() & df['job_title'].isnull() & df['level'].isnull() & df['descriptions'].isnull())]\n",
    "        df = df[~df.index.duplicated(keep='first')]\n",
    "        logger.info(f'Removed {wNulls - len(df)} empty rows')\n",
    "\n",
    "    # Deduplication, some jobs are similar but have different link maybe due to their different posting time / reposting\n",
    "    if remove_duplicates:\n",
    "        # subset_duplicates = ['company', 'job_title', 'level', 'job_type', 'degree', 'experience', 'spark', 'descriptions', 'industry1']\n",
    "        wDups = len(df)\n",
    "        df = df.drop_duplicates(subset=['job_id'])\n",
    "        logger.info(f'Removed {wDups - len(df)} duplicates')\n",
    "\n",
    "    # Sorting order and values\n",
    "    df = df[['company', 'job_title', 'level', 'job_type', 'experience', 'spark', 'degree', 'descriptions', 'industry1', 'industry2', 'link']]\n",
    "    df = df.sort_values(by=['company', 'job_title', 'level'],\n",
    "                        ascending= [True, True, True])\n",
    "    logger.info(f'Extracted {len(df)} number of jobs')\n",
    "\n",
    "    return df\n",
    "\n",
    "def append_to_main(main_df_filepath: str, most_recent_filepaths: list)-> pd.DataFrame:\n",
    "    '''\n",
    "    Function to append a list of excels to a main excel file\n",
    "    Inputs:\n",
    "        main_df_filepath: str - filepath to the main file\n",
    "        most_recent_filepaths: list - list of filepaths to iterate through\n",
    "    Returns:\n",
    "        pd.DataFrame: a compiled dataframe\n",
    "    '''\n",
    "\n",
    "\n",
    "    # Appending to existing dataframe\n",
    "    subfolder = 'ignore/'\n",
    "    main_df = pd.read_excel(subfolder+main_df_filepath)\n",
    "    # subset_duplicates = ['company', 'job_title', 'level', 'job_type', 'degree', 'experience', 'spark', 'descriptions', 'industry1']\n",
    "    original_rows = len(main_df)\n",
    "    logger.info(f'Original rows: {original_rows}')\n",
    "\n",
    "    if most_recent_filepaths:\n",
    "        for filepath in most_recent_filepaths:\n",
    "            if 'MAIN' not in filepath:\n",
    "                sub_df = pd.read_excel(subfolder+filepath, index_col=0)\n",
    "                # sub_df.insert(2, 'Emailed  / Messaged Recruiter', np.nan)\n",
    "                # sub_df.insert(2, 'Apply', np.nan)\n",
    "                logger.info(f'Read rows: {len(sub_df)}')\n",
    "                main_df = pd.concat([main_df, sub_df])\n",
    "                # main_df = main_df.drop_duplicates(subset=subset_duplicates)\n",
    "                main_df = main_df[~main_df.index.duplicated(keep='first')] # This drops by index instead\n",
    "\n",
    "    else:\n",
    "        excel_filepaths = [file for file in os.listdir('ignore') if file.endswith('xlsx')]\n",
    "        for item in enumerate(excel_filepaths):\n",
    "            print(item)\n",
    "        idx = input(\"Select file index: \")\n",
    "        sub_df = pd.read_excel(subfolder+excel_filepaths[idx], index_col=0)\n",
    "        # sub_df.insert(2, 'Emailed  / Messaged Recruiter', np.nan)\n",
    "        # sub_df.insert(2, 'Apply', np.nan)\n",
    "        logger.info(f'Read rows: {len(sub_df)}')\n",
    "        main_df = pd.concat([main_df, sub_df])\n",
    "        # main_df = main_df.drop_duplicates(subset=subset_duplicates)\n",
    "        main_df = main_df[~main_df.index.duplicated(keep='first')] # This drops by index instead\n",
    "    \n",
    "    logger.info(f'Added rows: {len(main_df)-original_rows}')\n",
    "    \n",
    "    return main_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input job search keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword = 'data analyst'\n",
    "pages = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test and modifications:\n",
    "# link = 'https://www.linkedin.com/jobs/view/data-analyst-digital-banking-at-seamoney-3779064039/?trackingId=aCJ2KUqm4mf0TIndAcY4zQ%3D%3D&refId=OHUthHp%2FYKwJ3V%2FzgKIwvQ%3D%3D&pageNum=0&position=16&trk=public_jobs_jserp-result_search-card&originalSubdomain=sg'\n",
    "# info, soup = get_job_info(link, return_soup=True)\n",
    "# descriptions = soup.find(class_ = \"show-more-less-html__markup show-more-less-html__markup--clamp-after-5 relative overflow-hidden\")\n",
    "# for tag in descriptions.contents:\n",
    "#     print(tag)\n",
    "# print(info['descriptions'])\n",
    "# info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:05<00:00,  1.00s/it]\n",
      "100%|██████████| 102/102 [01:59<00:00,  1.17s/it]\n"
     ]
    }
   ],
   "source": [
    "links, urls = get_job_links(keyword, start_page=0, pages=pages)\n",
    "main = {}\n",
    "soups = {}\n",
    "logger.info('Getting job info')\n",
    "for index, link in tqdm(enumerate(links), total = len(links), dynamic_ncols =True):\n",
    "    main[index], soups[index] = get_job_info(link, index, return_soup=True)\n",
    "# for index, link in enumerate(links):\n",
    "#     main[index] = get_job_info(link, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# soups[88]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>job_title</th>\n",
       "      <th>level</th>\n",
       "      <th>job_type</th>\n",
       "      <th>experience</th>\n",
       "      <th>spark</th>\n",
       "      <th>degree</th>\n",
       "      <th>descriptions</th>\n",
       "      <th>industry1</th>\n",
       "      <th>industry2</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>job_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3716933612</th>\n",
       "      <td>4e Exchange</td>\n",
       "      <td>Senior Data Analyst</td>\n",
       "      <td>Mid-Senior level</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>•        4e Exchange, a leading financial deri...</td>\n",
       "      <td></td>\n",
       "      <td>•Bachelor's or Master's degree in Data Science...</td>\n",
       "      <td>4e Exchange, a leading financial deriv...</td>\n",
       "      <td>Analyst</td>\n",
       "      <td>Technology, Information and Internet</td>\n",
       "      <td>https://sg.linkedin.com/jobs/view/senior-data-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3746296451</th>\n",
       "      <td>AbbVie</td>\n",
       "      <td>Business Data Analyst</td>\n",
       "      <td>Mid-Senior level</td>\n",
       "      <td>Full-time</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>•Use appropriate analytics platforms and analy...</td>\n",
       "      <td>\\nJob Overview:\\nThis position is responsible ...</td>\n",
       "      <td>Project Management, Other, and Information Tec...</td>\n",
       "      <td>Pharmaceutical Manufacturing and Biotechnology...</td>\n",
       "      <td>https://sg.linkedin.com/jobs/view/business-dat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3757235292</th>\n",
       "      <td>Allianz Global Investors</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Entry level</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>•We encourage candidates with at least 3 years...</td>\n",
       "      <td></td>\n",
       "      <td>•University degree in computer science, IT, fi...</td>\n",
       "      <td>\\nJoin us. Let’s care for tomorrow.\\nAt Allian...</td>\n",
       "      <td>Information Technology</td>\n",
       "      <td>Financial Services and Insurance</td>\n",
       "      <td>https://sg.linkedin.com/jobs/view/data-analyst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3775716634</th>\n",
       "      <td>Arta Finance</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Entry level</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>•Bachelor's degree. MBA is a plus.Minimum 3 ye...</td>\n",
       "      <td></td>\n",
       "      <td>•Bachelor's degree. MBA is a plus.Minimum 3 ye...</td>\n",
       "      <td>Arta Finance is a dynamic fintech star...</td>\n",
       "      <td>Information Technology</td>\n",
       "      <td>Financial Services</td>\n",
       "      <td>https://sg.linkedin.com/jobs/view/data-analyst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3780122428</th>\n",
       "      <td>Big Cloud</td>\n",
       "      <td>Senior Data Analyst</td>\n",
       "      <td>Mid-Senior level</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>•Are you experienced within data analytics?\\n</td>\n",
       "      <td></td>\n",
       "      <td>•Bachelor's degree in statistics/ business ana...</td>\n",
       "      <td>\\nAre you experienced within data analytics?\\n...</td>\n",
       "      <td>Engineering and Information Technology</td>\n",
       "      <td>Transportation, Logistics, Supply Chain and St...</td>\n",
       "      <td>https://sg.linkedin.com/jobs/view/senior-data-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             company              job_title             level  \\\n",
       "job_id                                                                          \n",
       "3716933612               4e Exchange    Senior Data Analyst  Mid-Senior level   \n",
       "3746296451                    AbbVie  Business Data Analyst  Mid-Senior level   \n",
       "3757235292  Allianz Global Investors           Data Analyst       Entry level   \n",
       "3775716634              Arta Finance           Data Analyst       Entry level   \n",
       "3780122428                 Big Cloud    Senior Data Analyst  Mid-Senior level   \n",
       "\n",
       "             job_type                                         experience  \\\n",
       "job_id                                                                     \n",
       "3716933612  Full-time  •        4e Exchange, a leading financial deri...   \n",
       "3746296451  Full-time                                                      \n",
       "3757235292  Full-time  •We encourage candidates with at least 3 years...   \n",
       "3775716634  Full-time  •Bachelor's degree. MBA is a plus.Minimum 3 ye...   \n",
       "3780122428  Full-time      •Are you experienced within data analytics?\\n   \n",
       "\n",
       "           spark                                             degree  \\\n",
       "job_id                                                                \n",
       "3716933612        •Bachelor's or Master's degree in Data Science...   \n",
       "3746296451        •Use appropriate analytics platforms and analy...   \n",
       "3757235292        •University degree in computer science, IT, fi...   \n",
       "3775716634        •Bachelor's degree. MBA is a plus.Minimum 3 ye...   \n",
       "3780122428        •Bachelor's degree in statistics/ business ana...   \n",
       "\n",
       "                                                 descriptions  \\\n",
       "job_id                                                          \n",
       "3716933612          4e Exchange, a leading financial deriv...   \n",
       "3746296451  \\nJob Overview:\\nThis position is responsible ...   \n",
       "3757235292  \\nJoin us. Let’s care for tomorrow.\\nAt Allian...   \n",
       "3775716634          Arta Finance is a dynamic fintech star...   \n",
       "3780122428  \\nAre you experienced within data analytics?\\n...   \n",
       "\n",
       "                                                    industry1  \\\n",
       "job_id                                                          \n",
       "3716933612                                            Analyst   \n",
       "3746296451  Project Management, Other, and Information Tec...   \n",
       "3757235292                             Information Technology   \n",
       "3775716634                             Information Technology   \n",
       "3780122428             Engineering and Information Technology   \n",
       "\n",
       "                                                    industry2  \\\n",
       "job_id                                                          \n",
       "3716933612               Technology, Information and Internet   \n",
       "3746296451  Pharmaceutical Manufacturing and Biotechnology...   \n",
       "3757235292                   Financial Services and Insurance   \n",
       "3775716634                                 Financial Services   \n",
       "3780122428  Transportation, Logistics, Supply Chain and St...   \n",
       "\n",
       "                                                         link  \n",
       "job_id                                                         \n",
       "3716933612  https://sg.linkedin.com/jobs/view/senior-data-...  \n",
       "3746296451  https://sg.linkedin.com/jobs/view/business-dat...  \n",
       "3757235292  https://sg.linkedin.com/jobs/view/data-analyst...  \n",
       "3775716634  https://sg.linkedin.com/jobs/view/data-analyst...  \n",
       "3780122428  https://sg.linkedin.com/jobs/view/senior-data-...  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = process_df(main, remove_nulls=True, remove_duplicates=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_recent_file = f\"{keyword}_{datetime.now().strftime('%Y-%m-%d-%H%M')}.xlsx\"\n",
    "df.to_excel(f\"ignore/{most_recent_file}\", engine='xlsxwriter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data analyst_2023-12-23-1602.xlsx']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_recent_filepaths = [file for file in os.listdir('ignore') if datetime.now().strftime('%Y-%m-%d') in file]\n",
    "most_recent_filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df = append_to_main('MAIN_2023-12-22.xlsx', most_recent_filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df.to_excel(f\"ignore/MAIN_{datetime.now().strftime('%Y-%m-%d')}.xlsx\", engine='xlsxwriter')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
